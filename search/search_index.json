{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Namah everyone  (1)</p> <ol> <li>Namah is \"Hello\" in Sanskrit: language. </li> </ol> <p>I am Vishesh Garg, a Passionate data guy.</p> <p>I will share my knowledge repository for almost 6+ years of my working experience around Data and especially Data Engineering, Problem Solving and About Interesting Data Pipelines. </p> <p>I have Currently working as Data Architect/Lead Data Engineer </p>"},{"location":"#overview","title":"Overview","text":"<p>Tip</p> <p>First, I bring up some exciting projects about data engineering, check it out here PROJECTS.</p> <p>I assure it's easy to follow even for beginners.</p> <p>If you want to deep dive into the topics and get basic/advance understandings, I suggest you go through the learning tabs, such as below:</p> <ul> <li>Data Engineering</li> <li>Data Modeling</li> <li>Data Architecture</li> <li>Data Orchestration</li> <li>Data Ingestion</li> <li>Data Processing</li> <li>Data Quality</li> <li>Learning Python</li> <li>Many more</li> </ul> <p>Stay tune to this notes, I will keep updating it to the most recent Data Engineering updates. Currently I am learning :</p> <ul> <li>AWS</li> <li>Pyspark</li> </ul> <p>Quote</p> <p>'Learning gives creativity, creativity leads to thinking, thinking provides knowledge, knowledge makes you great.' - Dr. A.P.J. Abdul Kalam</p> <p>In the same way, a robust data architecture is the foundation for organizational greatness. It enables learning by providing a single source of truth for data-driven insights. This, in turn, sparks creativity in business decision-making, as users can explore new possibilities and opportunities. The thinking that follows leads to knowledge, as the organization gains a deeper understanding of its customers, markets, and operations. And ultimately, this knowledge is what makes the organization great, as it informs strategic decisions and drives innovation and growth.</p> <p>Find my profile in About the author.</p>"},{"location":"author/","title":"About Me","text":"<p>Hello, I am Vishesh Garg </p> <p>Data Enthusiast having total 10+ years with relevant of 6+ in GCP, Data Pipeline Engineer, Data Engineering, Business Intelligence, Data lifecycle, big data/cloud technologies and customer engagements as Lead Data Engineer</p> <p>I always have a growth mindset and purpose to democratize access to data, provide analytics as a platform, and building data driven framework.</p> <p>Some of technical skills that I'm capable of: SQL, Python, Airflow,Data Designs, BigQuery, GCP, dimensional modeling, PySpark, Hive, Shell, many more.</p> <p>Check my LinkedIn for getting to know.</p>"},{"location":"author/#projects-data-security-platform","title":"Projects :Data Security Platform :","text":""},{"location":"author/#build-near-realtime-ingestion-from-various-apis-into-gcp-bigquery","title":"Build Near Realtime Ingestion from Various API's into GCP BigQuery","text":"<p>Company: Palo Alto Networks</p> <ul> <li>Designed a Real Time Data Pipeline using multiple PubSub Topics ,cloud functions &amp; BigQuery.</li> <li>Optimized the pipeline and reduce the total ingestion time by 200%.</li> <li>Created the design pattern on how to implement jobs in DAG, so it's testable and scalable, which increase reliability.</li> <li>Implemented data quality checks on all the tables to reduce data issues.</li> <li>Integrated pipeline with Atlantis for CI/CD </li> </ul> <p>Skills: REST API, Google PubSub, Cloud Functions, BigQuery, CDC, BQ Scheduled Queries etc.</p>"},{"location":"author/#data-ingestion-using-nexla-sharepoint-apis-custom-airflow-operators","title":"Data ingestion using Nexla, Sharepoint API's &amp; custom airflow Operators","text":"<p>Company: Intersoft Data Labs</p> <ul> <li>Developed and optimized various SQL Transformations using multiple aggregated functions in SQL for business analysts.</li> <li>Created airflow custom operators and Python functions to use in the existing composer framework.</li> <li>Created multiple data ingestion flows using Nexla (3rd party tool) into BQ in Composer using Python.</li> <li>Converted legacy data flows into Stored Procedure-driven data flows for BigQuery.</li> </ul> <p>Skills: DBT, dimensional modeling, Airflow ,Stored Procedure, SQL, Unix etc</p>"},{"location":"author/#data-modeling-and-orchestration-for-customer-experience-team","title":"Data Modeling and Orchestration for Customer Experience team","text":"<p>Company: Mavenwave Partners </p> <ul> <li>Build Data Pipeline for migrating 10 TB from Hadoop,Terdata,DB2 systems into BQ using SQL Jinja templates.</li> <li>Used Dataproc for connecting the source systems from the On-prem systems into GCS.</li> <li>Developed Infra and ingestion DAG's into Cloud Composer for Orchestration Purpose.</li> <li>Developed a small POC on  Snowflake for data ingestion.</li> </ul> <p>Skills: DBT, Pyspark, Snowflake, dimensional modeling, GCP, Airflow</p>"},{"location":"author/#gcp-indepth-cost-optimizations-and-security-model","title":"GCP Indepth Cost Optimizations and Security Model","text":"<p>Company: DateMetica/Cardinal Health</p> <ul> <li>Achieved CDC using Dataflow and BQ Merge for the batch pipeline</li> <li>Developed JSON parser/extraction tool in Python for data extraction</li> <li>Set up BQ monitoring for Cost &amp; alerting using Custom metrics in Stackdriver &amp; GCP Datastudio having BQ as a data source.</li> </ul> <p>Skills: ETL, Airflow, Docker GCP, Python, Shell</p>"},{"location":"author/#_1","title":"About Me \ud83d\udc64","text":""},{"location":"projects/","title":"List of Projects","text":"<p>Hi, you will find list of projects which I had worked on so far:</p> <ul> <li>Getting Emails from Gmail &amp; Ingest into Google Cloud Storage</li> <li>Automated Data Quality Testing with Great Expectations and BigQuery</li> <li>Deploying and Running Airflow on Kubernetes</li> <li>Building Github Pages using Github Actions</li> <li>Running Apache Flink on Kubernetes</li> <li>Running Spark using Docker Compose</li> <li>Querying Streaming Data in Kafka using ksql</li> <li>Apache Iceberg and PySpark</li> <li>Delta Lake with Apache Spark</li> <li>Apache Hudi and Spark</li> <li>DuckDB Quick Start using Jupyter Notebook</li> </ul> <p>Stay tune, I will keep adding this list.</p>"},{"location":"projects/great-expectations-bq/","title":"Automated Data Quality with Great Expectations and BigQuery","text":"<p>must have</p> <p>Before starting this project, you need to install some prerequisites in your laptop:</p> <ul> <li>Docker</li> <li>gcloud CLI</li> <li>GCP console</li> </ul> <p>If you haven't create any project in Google Cloud Platform, please follow this</p>"},{"location":"projects/great-expectations-bq/#great-expectations","title":"Great Expectations","text":"<p>Great Expectations(GX) is a open source tool to have automated data quality testing, it can integrate with a lot of other tools/platforms.</p>"},{"location":"projects/great-expectations-bq/#advantages","title":"Advantages","text":"<ul> <li>Get started quickly</li> </ul> <p>use GX OSS with tools you're familiar such as Python and Jupyter notebook</p> <ul> <li>Create a shared point of view</li> </ul> <p>using GX OSS gives everyone a shared toolset and starting point while still allowing each team their own flexibility and independence.</p> <ul> <li>Communicate better</li> </ul> <p>Data Docs make it easy to work with stakeholders by automatically rendering business-ready visualizations and test results expressed in plain language.</p> <ul> <li>Take action</li> </ul> <p>Your data systems never sleep, so your data quality processes can\u2019t either. Take proactive action 24/7 with GX OSS: it integrates with your orchestrator to enable automation that ensures your data never goes unchecked.</p> <ul> <li>It works with the tools we know and love</li> </ul> <p>Apache Airflow, Amazon S3, databricks, Google Cloud Platform, Jupyter Notebook, PostgreSQL, many more.</p>"},{"location":"projects/great-expectations-bq/#the-power-of-expectations","title":"The power of <code>Expectations</code>","text":"<p>It's using <code>Expectation</code> as how they are calling their test cases.</p> <ul> <li>An Expectation is a verifiable assertion about your data.</li> </ul> <p>Creating an Expectation offers you unparalleled flexibility and control compared with other ways of defining data quality tests.</p> <ul> <li>This intuitive approach is accessible to technical and nontechnical teams.</li> </ul> <p>Expectations automatically generate updated documentation on top of individual validation results, making sure everyone has visibility into your test suites and validation results.</p> <ul> <li>Expectations offer deeper insights than schema-focused validation</li> </ul> <p>more resilient to changing business and technical requirements than low-configuration options like anomaly detection.</p> <ul> <li>Expectations are reusable and can be auto-generated</li> </ul> <p>making it easy to deploy them across large amounts of data</p>"},{"location":"projects/great-expectations-bq/#build-great-expectations-docker-image","title":"Build Great Expectations docker image","text":"<ul> <li>clone the repo</li> </ul> <pre><code>git clone git@github.com:karlchris/great-expectations-bq.git\n</code></pre> <ul> <li>make sure you are in <code>data-engineering dir</code>, then run this command</li> </ul> <pre><code>make build-gx\n</code></pre> <p>Output:</p> <pre><code>\u279c  data-engineering git:(great-expectations) make build-gx\nBuilding great expectations image\n[+] Building 2.1s (10/10) FINISHED                               docker:desktop-linux\n =&gt; [internal] load build definition from Dockerfile                             0.0s\n =&gt; =&gt; transferring dockerfile: 315B                                             0.0s\n =&gt; [internal] load metadata for docker.io/library/python:3.9                    1.9s\n =&gt; [auth] library/python:pull token for registry-1.docker.io                    0.0s\n =&gt; [internal] load .dockerignore                                                0.0s\n =&gt; =&gt; transferring context: 2B                                                  0.0s\n =&gt; [1/4] FROM docker.io/library/python:3.9@sha256:1446afd121c574b13077f4137443  0.0s\n =&gt; [internal] load build context                                                0.0s\n =&gt; =&gt; transferring context: 4.19MB                                              0.0s\n =&gt; CACHED [2/4] WORKDIR /app                                                    0.0s\n =&gt; CACHED [3/4] RUN pip install great-expectations==0.18.15 &amp;&amp;     pip install  0.0s\n =&gt; [4/4] COPY . /app                                                            0.1s\n =&gt; exporting to image                                                           0.0s\n =&gt; =&gt; exporting layers                                                          0.0s\n =&gt; =&gt; writing image sha256:03928fb4d00f8d5a26079b81d01e4df586e116ee1e3f05fda18  0.0s\n =&gt; =&gt; naming to docker.io/library/gx                                            0.0s\n</code></pre> <ul> <li>If you see above output, then your great expectations image is alrady built.</li> </ul>"},{"location":"projects/great-expectations-bq/#run-great-expectations-container-interactively","title":"Run Great Expectations container interactively","text":"<ul> <li>run this command</li> </ul> <pre><code>make run-gx\n</code></pre> <p>Output:</p> <pre><code>\u279c  data-engineering git:(great-expectations) \u2717 make run-gx\nRunning great expectations in container\nroot@c6ee28c3a1f6:/app#\n</code></pre> <ul> <li>run this command to initiate the <code>DataContext</code>, connecting to <code>Datasource</code> and running some tests (tests are called as <code>Expectations</code>)</li> </ul> <pre><code>from ruamel import yaml\n\nimport great_expectations as ge\nfrom great_expectations.core.batch import RuntimeBatchRequest\n\nGCP_PROJECT_NAME = \"data-engineering-424915\"\nBIGQUERY_DATASET = \"data_eng\"\n\n\n# Instantiate project DataContext\ncontext = ge.get_context()\n\n# Configure Datasource\ndatasource_config = {\n    \"name\": \"my_bigquery_datasource\",\n    \"class_name\": \"Datasource\",\n    \"execution_engine\": {\n        \"class_name\": \"SqlAlchemyExecutionEngine\",\n        \"connection_string\": f\"bigquery://{GCP_PROJECT_NAME}/{BIGQUERY_DATASET}\",\n    },\n    \"data_connectors\": {\n        \"default_runtime_data_connector_name\": {\n            \"class_name\": \"RuntimeDataConnector\",\n            \"batch_identifiers\": [\"default_identifier_name\"],\n        },\n        \"default_inferred_data_connector_name\": {\n            \"class_name\": \"InferredAssetSqlDataConnector\",\n            \"include_schema_name\": True,\n        },\n    },\n}\ncontext.test_yaml_config(yaml.dump(datasource_config))\n\n# Save Datasource configuration to DataContext\ncontext.add_datasource(**datasource_config)\n\n# Test new Datasource\nbatch_request = RuntimeBatchRequest(\n    datasource_name=\"my_bigquery_datasource\",\n    data_connector_name=\"default_runtime_data_connector_name\",\n    data_asset_name=\"table_sales\",  # this can be anything that identifies this data\n    runtime_parameters={\"query\": \"SELECT * from data_eng.table_sales LIMIT 10\"},\n    batch_identifiers={\"default_identifier_name\": \"default_identifier\"},\n)\n\ncontext.add_or_update_expectation_suite(\n    expectation_suite_name=\"test_suite\"\n)\nvalidator = context.get_validator(\n    batch_request=batch_request, expectation_suite_name=\"test_suite\"\n)\nprint(validator.head())\n\n# Data Quality checks\nvalidator.expect_column_values_to_not_be_null(\"quantity\")\n\nvalidator.expect_column_values_to_not_be_null(\"product\")\nvalidator.expect_column_values_to_be_in_set(\n    \"product\",\n    [\"apple\", \"pear\", \"banana\",],\n)\n\nvalidator.expect_column_values_to_not_be_null(\"price\")\nvalidator.expect_column_values_to_be_between(\n    \"price\", min_value=0, max_value=None,\n)\nvalidator.save_expectation_suite(discard_failed_expectations=False)\n\n# Run checkpoint\ncheckpoint = context.add_or_update_checkpoint(\n    name=\"my_quickstart_checkpoint\",\n    validator=validator,\n)\ncheckpoint_result = checkpoint.run()\ncontext.view_validation_result(checkpoint_result)\n</code></pre> <p>you need to run python script above using</p> <pre><code>python run.py\n</code></pre> <p>Output:</p> <pre><code>root@9e909c33e977:/app# python run.py\nAttempting to instantiate class from config...\n        Instantiating as a Datasource, since class_name is Datasource\n        Successfully instantiated Datasource\n\n\nExecutionEngine class name: SqlAlchemyExecutionEngine\nData Connectors:\n        default_inferred_data_connector_name : InferredAssetSqlDataConnector\n\n        Available data_asset_names (3 of 10):\n                data_eng.gx_temp_29379392 (1 of 1): [{}]\n                data_eng.gx_temp_31f38543 (1 of 1): [{}]\n                data_eng.gx_temp_78a977d7 (1 of 1): [{}]\n\n        Unmatched data_references (0 of 0):[]\n\n        default_runtime_data_connector_name:RuntimeDataConnector\n\n        Available data_asset_names (0 of 0):\n                Note : RuntimeDataConnector will not have data_asset_names until they are passed in through RuntimeBatchRequest\n\n        Unmatched data_references (0 of 0): []\n\nCalculating Metrics: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00,  1.61it/s]\n  product  quantity  price                      date\n0   apple        10    1.0 2023-01-01 00:00:00+00:00\n1  banana         7    3.0 2023-01-01 00:00:00+00:00\n2    pear         5    2.0 2023-01-01 00:00:00+00:00\nCalculating Metrics: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8/8 [00:02&lt;00:00,  3.30it/s]\nCalculating Metrics: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8/8 [00:02&lt;00:00,  2.69it/s]\nCalculating Metrics: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:03&lt;00:00,  3.63it/s]\nCalculating Metrics: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8/8 [00:02&lt;00:00,  2.80it/s]\nCalculating Metrics: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:02&lt;00:00,  4.63it/s]\nCalculating Metrics: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 34/34 [00:05&lt;00:00,  6.17it/s]\n</code></pre> <ul> <li>all the <code>Expectations</code> have been built and tested, the result will be stored in <code>gx</code> dir.</li> </ul>"},{"location":"projects/great-expectations-bq/#data-docs-in-ui","title":"Data docs in UI","text":"<ul> <li>run this command</li> </ul> <pre><code>great_expectations docs build\n</code></pre> <p>Output:</p> <pre><code>root@12a2f87f7cb7:/app# great_expectations docs build\n\nThe following Data Docs sites will be built:\n\n - local_site: file:///app/gx/uncommitted/data_docs/local_site/index.html\n\nWould you like to proceed? [Y/n]: Y\n\nBuilding Data Docs...\n\nDone building Data Docs\n</code></pre> <ul> <li>the UI will be saved as HTML file in your local path <code>file:///app/gx/uncommitted/data_docs/local_site/index.html</code></li> </ul> <p>List of expectations suite</p> <p></p> <p>Test results</p> <p></p> <ul> <li>Done, you have wrapped data quality testing using Great Expectations to run within Docker.</li> </ul> <p>Tip</p> <p>Next step, you can schedule this to run in any server or cloud providers.</p> <p>Since it's already in Docker, it's easier to ship it in anywhere.</p> <p>Reference: Great Expectations: Connecting to BigQuery</p>"}]}