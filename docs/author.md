# About Me :bust_in_silhouette:

![Vishesh photo](assets/img/photo_vishesh_yatch_page-0001.jpg)

_Hello_, I am **Vishesh Garg** :wave:

Data Enthusiast having total 10+ years with relevant of 6+ in GCP, Data Pipeline Engineer, Data Engineering, Business Intelligence, Data lifecycle, big data/cloud technologies and customer engagements as Lead Data Engineer

I always have a **growth mindset** and purpose to democratize access to **data**, provide analytics as a platform, and building **data driven** framework.

Some of technical skills that I'm capable of: **SQL, Python, Airflow,Data Designs, BigQuery, GCP, dimensional modeling, PySpark, Hive, Shell, many more.**

> Check my [LinkedIn](https://www.linkedin.com/in/vishesh196) for getting to know.

## Projects :Data Security Platform :

### Build Near Realtime Ingestion from Various API's into GCP BigQuery
Company: Palo Alto Networks

- Designed a **Real Time Data Pipeline** using multiple PubSub Topics ,cloud functions & BigQuery.
- **Optimized the pipeline** and reduce the total ingestion time by **200%**.
- Created the **design pattern** on how to implement jobs in **DAG**, so it's testable and scalable, which **increase reliability**.
- Implemented **data quality checks** on all the tables to reduce data issues.
- **Integrated pipeline** with **Atlantis** for CI/CD 

> Skills: REST API, Google PubSub, Cloud Functions, BigQuery, CDC, BQ Scheduled Queries etc.

### Data ingestion using Nexla, Sharepoint API's & custom airflow Operators

Company: Intersoft Data Labs

- Developed and optimized various SQL Transformations using multiple aggregated functions in SQL for business analysts.
- Created airflow custom operators and Python functions to use in the existing composer framework.
- Created multiple data ingestion flows using **Nexla** (3rd party tool) into BQ in Composer using Python.
- Converted legacy data flows into **Stored Procedure**-driven data flows for BigQuery.

> Skills: DBT, dimensional modeling, Airflow ,Stored Procedure, SQL, Unix etc

### Data Modeling and Orchestration for Customer Experience team

Company: Mavenwave Partners 

- Build Data Pipeline for migrating **10 TB** from Hadoop,Terdata,DB2 systems into **BQ** using SQL Jinja templates.
- Used **Dataproc** for connecting the source systems from the On-prem systems into GCS.
- Developed Infra and ingestion DAG's into Cloud Composer for **Orchestration** Purpose.
- Developed a small POC on  Snowflake for data ingestion.

> Skills: DBT, Pyspark, Snowflake, dimensional modeling, GCP, Airflow

### GCP Indepth Cost Optimizations and Security Model

Company: DateMetica/Cardinal Health

- Achieved CDC using Dataflow and **BQ Merge** for the batch pipeline
- Developed JSON parser/extraction tool in Python for data extraction
- Set up BQ monitoring for Cost & alerting using **Custom metrics** in Stackdriver & GCP Datastudio having BQ as a data source.

> Skills: ETL, Airflow, Docker GCP, Python, Shell

### 